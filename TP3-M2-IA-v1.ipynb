{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP N°3 - Apprentissage automatique\n",
    "### Le Perceptron multicouche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Chargement des données de MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertion des bibliothèques\n",
    "import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de numpy est : 1.26.4\n"
     ]
    }
   ],
   "source": [
    "print(\"La version de numpy est :\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "mnistData = mnist.MNIST(\"Data\\\\MNIST\") # Path of the four ubyte files.\n",
    "trainData, trainLabels = mnistData.load_training()\n",
    "testData, testLabels = mnistData.load_testing()\n",
    "trainData = np.array(trainData)\n",
    "trainLabels = np.array(trainLabels).reshape(len(trainLabels), 1)\n",
    "testData = np.array(testData)\n",
    "testLabels = np.array(testLabels).reshape(1, len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "..............@@@@@@........\n",
      "...........@@@@@@@@@@.......\n",
      "..........@@@@@@@@@@@@......\n",
      "..........@@@.....@@@@......\n",
      "..................@@@.......\n",
      "..................@@@.......\n",
      "..................@@@.......\n",
      "...............@@@@@@.......\n",
      "..............@@@@@.........\n",
      ".........@@@@@@@@@..........\n",
      ".........@@@@@@@@@@.........\n",
      ".................@@.........\n",
      ".................@@.........\n",
      ".................@@.........\n",
      "................@@@.........\n",
      ".......@.......@@@@.........\n",
      ".....@@@.....@@@@...........\n",
      ".....@@@@@@@@@@@............\n",
      ".......@@@@@@@..............\n",
      "........@@..................\n",
      "............................\n",
      "............................\n",
      "............................\n"
     ]
    }
   ],
   "source": [
    "# Visualisation du contenu d’une image \n",
    "\n",
    "print(mnist.MNIST().display(trainData[7])) # I is the index of the image et peut être n'importe quel chifre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat obtenu n'est pas fiable/correcte. Une modification est nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Normalisation des données\n",
    "\n",
    "La méthode d’optimisation liée au Perceptron multicouche (c.-à-d. la descente de gradient),\n",
    "est optimale lorsque les données sont dans un interval particulier Xn ∈ [0, 1]. Ainsi, normalisez\n",
    "les données des bases WDBC et MNIST par leur maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "mnistData = mnist.MNIST(\"Data\\\\MNIST\") # Path of the four ubyte files.\n",
    "trainData, trainLabels = mnistData.load_training()\n",
    "testData, testLabels = mnistData.load_testing()\n",
    "trainData = np.array(trainData)/255 # afin de diviser par des floatant toujours entre 0 et 1\n",
    "trainLabels = np.array(trainLabels).reshape(len(trainLabels), 1)\n",
    "testData = np.array(testData)/255\n",
    "testLabels = np.array(testLabels).reshape(1, len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabelsMatrix = np.zeros((trainLabels.size, 10))\n",
    "for n in range(trainLabels.size):\n",
    "    trainLabelsMatrix[n, trainLabels[n]] = 1\n",
    "    \n",
    "# return trainData, trainLabelsMatrix, testData, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n",
      "............................\n"
     ]
    }
   ],
   "source": [
    "# Visualisation du contenu d’une image \n",
    "\n",
    "print(mnist.MNIST().display(trainData[4])) # I is the index of the image et peut être n'importe quel chifre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04705882\n",
      " 0.38823529 0.35686275 0.55686275 0.60784314 0.96470588 0.71372549\n",
      " 0.60784314 0.60784314 0.60784314 0.60784314 0.51372549 0.20392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54117647 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.98823529 0.82352941 0.47843137\n",
      " 0.12941176 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.8627451  0.99607843 0.99607843 0.99607843 0.92156863\n",
      " 0.74117647 0.74117647 0.74117647 0.74117647 0.58823529 0.74117647\n",
      " 0.80392157 0.99607843 0.99607843 0.99607843 0.29411765 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.1372549\n",
      " 0.29019608 0.1372549  0.1372549  0.09803922 0.         0.\n",
      " 0.         0.         0.         0.         0.05098039 0.87843137\n",
      " 0.99607843 0.99607843 0.6        0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.35294118 0.99607843 0.99607843 0.96862745\n",
      " 0.20784314 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02352941 0.59607843\n",
      " 0.96470588 0.99607843 0.99607843 0.19215686 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.25882353 0.61960784 0.99607843 0.99607843 0.97647059\n",
      " 0.40392157 0.03137255 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.21176471 0.98431373\n",
      " 0.99607843 0.99607843 0.99607843 0.97254902 0.29019608 0.01960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901961 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.79215686 0.49019608 0.17647059\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.22745098 0.70980392 0.91764706 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.98823529 0.54901961 0.08627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11764706 0.19607843 0.28627451 0.60784314 0.99215686 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.74901961 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.35686275 0.78431373 0.99607843 0.99607843\n",
      " 0.99607843 0.99607843 0.4627451  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01568627 0.75294118 0.99607843 0.99607843 0.99607843\n",
      " 0.60392157 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.55294118 0.99607843 0.99607843 0.99607843 0.45490196 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09803922 0.49411765 0.3372549  0.         0.         0.\n",
      " 0.         0.         0.         0.01176471 0.7372549  0.99607843\n",
      " 0.99607843 0.98039216 0.23921569 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.09411765 0.81960784 0.99607843\n",
      " 0.05882353 0.         0.         0.         0.         0.\n",
      " 0.09019608 0.5372549  0.99607843 0.99607843 0.99607843 0.81960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.65882353 0.99607843 0.99607843 0.18823529 0.03529412\n",
      " 0.         0.         0.03529412 0.49803922 0.94509804 0.99607843\n",
      " 0.99607843 1.         0.94901961 0.24705882 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.39607843\n",
      " 0.99607843 0.99607843 0.99607843 0.80392157 0.74509804 0.74509804\n",
      " 0.80392157 0.99607843 0.99607843 0.99607843 0.99607843 0.94901961\n",
      " 0.2627451  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12941176 0.65098039 0.99607843\n",
      " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.98039216 0.54117647 0.21568627 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.02745098 0.34509804 0.60392157 0.45490196\n",
      " 0.76078431 0.76078431 0.60392157 0.60392157 0.34509804 0.19215686\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def readMnist(mnistPath) : \n",
    "    mnistData = mnist.MNIST(mnistPath) # Path of the four ubyte files.\n",
    "    trainData, trainLabels = mnistData.load_training()\n",
    "    testData, testLabels = mnistData.load_testing()\n",
    "    trainData = np.array(trainData)/255 # afin de diviser par des floatant toujours entre 0 et 1\n",
    "    trainLabels = np.array(trainLabels).reshape(len(trainLabels), 1)\n",
    "    testData = np.array(testData)/255\n",
    "    testLabels = np.array(testLabels).reshape(1, len(testLabels))\n",
    "    trainLabelsMatrix = np.zeros((trainLabels.size, 10))\n",
    "    for n in range(trainLabels.size):\n",
    "        trainLabelsMatrix[n, trainLabels[n]] = 1\n",
    "        \n",
    "    return trainData, trainLabelsMatrix, testData, testLabels\n",
    "\n",
    "trainData, trainLabels, testData, testLabels = readMnist(\"Data\\\\MNIST\")\n",
    "print(trainData[12, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Fonction d’activation\n",
    "\n",
    "Afin que le réseau de neurones apprenne, il est nécessaire de définir une fonction d’activation.\n",
    "Dans notre cas, nous utiliserons la fonction sigmoïde, voir équation 1 ainsi que sa dérivée pour\n",
    "l’étape de back-propagation, voir équation 2. Où derivate vaut False si on souhaite calculer f(x) et True si on souhaite calculer f′(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivate=False):\n",
    "    fx = 1/(1+ np.exp(-x))\n",
    "    if derivate:\n",
    "        return fx*(1 - fx)\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Apprentissage du réseau de neurones\n",
    "\n",
    "Dans cette partie, on souhaite définir le réseau de neurones et l’entraîner sur les données d’entraînement. Pour un meilleur apprentissage, nous utiliserons la technique du batch-processing.\n",
    "\n",
    "Notre réseau de neurones sera défini comme suit :\n",
    "\n",
    "• Nombre de neurones d’entrées : 3 pour WDBC - 28 ∗ 28 = 784 pour MNIST.\n",
    "\n",
    "• Nombre de couches cachées : 1.\n",
    "\n",
    "• Nombre de neurones dans le niveau caché : 20 pour WDBC - 30 pour MNIST.\n",
    "\n",
    "• Nombre de neurones de sorties : 1 pour WDBC - 10 pour MNIST.\n",
    "\n",
    "\n",
    "Afin de créer ce réseau, définissez la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNeuralNetwork(input, output, hiddenLayerSize, batchSize, learningRate,numberOfIterations):\n",
    "    \"\"\" Train neural network\"\"\"\n",
    "    inputSize = input.shape[1]\n",
    "    outputSize = output.shape[1]\n",
    "    errors = np.zeros(numberOfIterations)\n",
    "    \n",
    "    # Initialisation des poids\n",
    "    weigths0 = np.random.rand(inputSize, hiddenLayerSize)/batchSize\n",
    "    weigths1 = np.random.rand(hiddenLayerSize, outputSize)/batchSize\n",
    "    \n",
    "    # Algorithme\n",
    "    for i in range(numberOfIterations):\n",
    "        # Calcul de la sortie de chaque couche \n",
    "        # Y l = f(qN i=1 WilXil)\n",
    "        layer0 = input\n",
    "        layer1 = sigmoid(layer0.dot(weigths0))\n",
    "        layer2 = sigmoid(layer1.dot(weigths1))\n",
    "        \n",
    "        \n",
    "        # Calcul de l’erreur d’apprentissage El\n",
    "        layer2Error = output - layer2\n",
    "        errors[i] = np.mean(np.sum(abs(layer2Error),1))\n",
    "        \n",
    "        # Rétropropagation de l’erreur par descente de gradient\n",
    "        layer2Delta = layer2Error*sigmoid(layer1.dot(weigths1),True)\n",
    "        layer1Error = layer2Delta.dot(weigths1.transpose())\n",
    "        layer1Delta = layer1Error*sigmoid(layer0.dot(weigths0),True)\n",
    "        \n",
    "        # Mise à jour des poids Wli\n",
    "        weigths0 += learningRate*layer0.transpose().dot(layer1Delta)\n",
    "        weigths1 += learningRate*layer1.transpose().dot(layer2Delta)\n",
    "    \n",
    "    return weigths0,weigths1,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elisabeth\\AppData\\Local\\Temp\\ipykernel_11816\\1238430488.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  fx = 1/(1+ np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.00663217, 0.0130846 , 0.00871711, ..., 0.0117892 , 0.00577801,\n",
       "         0.00245706],\n",
       "        [0.00151639, 0.01087824, 0.01748484, ..., 0.00269502, 0.0036236 ,\n",
       "         0.00932723],\n",
       "        [0.00088593, 0.01404495, 0.01458021, ..., 0.01293558, 0.00181888,\n",
       "         0.00230094],\n",
       "        ...,\n",
       "        [0.01643923, 0.01331976, 0.01357155, ..., 0.00827732, 0.01626867,\n",
       "         0.00290808],\n",
       "        [0.00732486, 0.00935086, 0.01122024, ..., 0.00045211, 0.01729031,\n",
       "         0.01611908],\n",
       "        [0.01755922, 0.00569855, 0.00072022, ..., 0.00354022, 0.01819503,\n",
       "         0.01903159]]),\n",
       " array([[-49011.38676352, -49412.49144745, -48217.51956861,\n",
       "         -48752.63287559, -50233.16012494, -49341.46809022,\n",
       "         -49823.02691954, -48382.57860896, -49112.09601438,\n",
       "         -49884.73989987],\n",
       "        [-49025.17440268, -49609.85856465, -48361.73236918,\n",
       "         -48907.25032662, -50382.11764237, -49435.58983981,\n",
       "         -49920.6839502 , -48488.60991787, -49257.44573935,\n",
       "         -50046.80607534],\n",
       "        [-49351.67430862, -49898.68853004, -48740.47404819,\n",
       "         -49256.88091343, -50862.75345129, -49818.93973558,\n",
       "         -50319.51571735, -48950.43284202, -49641.2443224 ,\n",
       "         -50494.9686365 ],\n",
       "        [-48626.45706625, -49136.71816033, -47987.52391416,\n",
       "         -48505.10258162, -50072.02687633, -49026.41358576,\n",
       "         -49531.61518029, -48153.52811144, -48788.71876685,\n",
       "         -49737.0033337 ],\n",
       "        [-48640.33750129, -49342.60145088, -48092.15155882,\n",
       "         -48613.91433393, -50103.95456088, -49069.78376581,\n",
       "         -49659.42503634, -48311.73406138, -48939.19100214,\n",
       "         -49791.051539  ],\n",
       "        [-48425.10735728, -49053.49180971, -47928.80263669,\n",
       "         -48357.81294732, -49936.25687706, -48861.33674219,\n",
       "         -49433.22244095, -48085.26733891, -48737.38122417,\n",
       "         -49605.21980338],\n",
       "        [-49092.77320213, -49442.48104776, -48323.74700227,\n",
       "         -48824.90086867, -50389.06778903, -49368.21437926,\n",
       "         -49962.6514944 , -48538.95180764, -49179.9446699 ,\n",
       "         -49991.27436892],\n",
       "        [-49762.5676309 , -50411.02650855, -49131.29764309,\n",
       "         -49584.97016431, -51199.3969726 , -50177.05429928,\n",
       "         -50653.91082234, -49368.86742318, -50000.82348155,\n",
       "         -50822.74248654],\n",
       "        [-48852.50344146, -49269.01894457, -48183.47339882,\n",
       "         -48681.16324433, -50200.9654558 , -49203.79652086,\n",
       "         -49758.18903849, -48305.29737055, -48970.09136435,\n",
       "         -49825.11231025],\n",
       "        [-48044.29926624, -48607.78605626, -47478.75325183,\n",
       "         -47905.4438926 , -49429.01576934, -48415.0580735 ,\n",
       "         -49036.79899149, -47573.59076067, -48308.50938414,\n",
       "         -49055.05725414],\n",
       "        [-49006.36548243, -49578.09863002, -48368.16134705,\n",
       "         -48877.19841578, -50513.40351616, -49402.39560076,\n",
       "         -49909.18767251, -48535.61824771, -49198.14432907,\n",
       "         -50061.41252624],\n",
       "        [-48960.10121987, -49463.90856141, -48303.9851913 ,\n",
       "         -48848.47604316, -50378.1523177 , -49339.76536284,\n",
       "         -49883.53450035, -48498.4857247 , -49131.8181988 ,\n",
       "         -50021.05073893],\n",
       "        [-48421.58042317, -48956.64589951, -47849.80404288,\n",
       "         -48343.50662256, -49883.62056189, -48863.25394421,\n",
       "         -49468.43514485, -48011.15436254, -48688.70291067,\n",
       "         -49566.56751268],\n",
       "        [-48752.89075078, -49184.03564825, -48012.28205617,\n",
       "         -48550.27976765, -50064.91764098, -49048.19641427,\n",
       "         -49600.8518336 , -48160.29183105, -48893.04215988,\n",
       "         -49726.2181637 ],\n",
       "        [-48999.25259947, -49463.17900341, -48326.84969684,\n",
       "         -48846.16825199, -50280.11284577, -49330.54546554,\n",
       "         -49893.74557909, -48399.46297091, -49169.8164516 ,\n",
       "         -49907.00547493],\n",
       "        [-49871.62182339, -50339.48244101, -49128.75574065,\n",
       "         -49631.65345269, -51296.10355974, -50201.93733798,\n",
       "         -50815.16784509, -49363.45586376, -49952.96974423,\n",
       "         -50887.20040192],\n",
       "        [-48682.79052631, -49291.7297648 , -48038.11637709,\n",
       "         -48535.02412553, -50069.73847002, -49068.95671698,\n",
       "         -49624.62591793, -48352.90856506, -48901.56656815,\n",
       "         -49808.2827397 ],\n",
       "        [-48347.58225939, -48963.98055875, -47653.59880567,\n",
       "         -48262.0661909 , -49706.34813511, -48762.93906865,\n",
       "         -49333.92466148, -47912.36284321, -48570.92500807,\n",
       "         -49389.01573177],\n",
       "        [-48660.9809636 , -49147.8994585 , -48066.35479408,\n",
       "         -48549.35835777, -50013.25738238, -49021.22121051,\n",
       "         -49563.05395411, -48262.84727051, -48885.42730752,\n",
       "         -49698.46200979],\n",
       "        [-49073.19420749, -49567.67531673, -48346.14406081,\n",
       "         -49006.78338709, -50448.8179271 , -49471.53992618,\n",
       "         -49817.32737981, -48590.09747502, -49299.76619712,\n",
       "         -50084.96170842],\n",
       "        [-48494.83492787, -48823.30647389, -47763.92409724,\n",
       "         -48366.99803456, -49780.9033851 , -48820.27954074,\n",
       "         -49209.27240423, -47942.4675737 , -48690.7531196 ,\n",
       "         -49479.80291685],\n",
       "        [-48912.93724181, -49417.63512992, -48285.77069898,\n",
       "         -48674.09917879, -50269.016514  , -49280.89600371,\n",
       "         -49941.69298861, -48400.08914677, -49046.61580596,\n",
       "         -49925.73471873],\n",
       "        [-49203.8860346 , -49522.31681177, -48340.32526221,\n",
       "         -48891.37938155, -50491.09148037, -49531.27649535,\n",
       "         -50009.64572029, -48614.99000415, -49342.7906316 ,\n",
       "         -50142.7095713 ],\n",
       "        [-48890.6906403 , -49376.62432249, -48204.62237251,\n",
       "         -48745.25229431, -50204.99025858, -49223.77040251,\n",
       "         -49790.58963908, -48274.16271669, -48958.6272407 ,\n",
       "         -49820.47174101],\n",
       "        [-48632.77558326, -49279.2001193 , -48047.9815601 ,\n",
       "         -48551.85514626, -50027.00732108, -49070.32230395,\n",
       "         -49476.09860687, -48271.50232594, -48976.6301454 ,\n",
       "         -49722.97864276],\n",
       "        [-48735.26560625, -49231.65872721, -48037.14584718,\n",
       "         -48528.06809787, -50084.66075129, -49091.24394628,\n",
       "         -49597.56054972, -48115.64809706, -48898.95725887,\n",
       "         -49686.04420972],\n",
       "        [-48479.78876613, -49133.95007448, -47870.96396373,\n",
       "         -48404.59595293, -49840.66643288, -48931.27058355,\n",
       "         -49541.58763085, -47927.32851823, -48733.04995729,\n",
       "         -49488.41801537],\n",
       "        [-49748.64686027, -50339.38371458, -49083.65334167,\n",
       "         -49606.30578514, -51123.77910709, -50140.15631993,\n",
       "         -50683.57597062, -49298.69632944, -49929.45572545,\n",
       "         -50787.83715403],\n",
       "        [-49605.5496679 , -50151.72275474, -49012.91246192,\n",
       "         -49567.3604791 , -50977.84402689, -50055.78506429,\n",
       "         -50652.54521515, -49132.74103532, -49849.05569534,\n",
       "         -50672.93217386],\n",
       "        [-48719.765358  , -49300.16407291, -48069.97550555,\n",
       "         -48537.02594101, -50032.65331148, -49085.02382576,\n",
       "         -49594.98981418, -48205.69074368, -48959.30742089,\n",
       "         -49679.61982175]]),\n",
       " array([5.42953623, 5.        , 5.        , 5.        , 5.        ]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData, trainLabels, testData, testLabels = readMnist(\"Data\\\\MNIST\")\n",
    "trainNeuralNetwork(trainData, trainLabels, 30 ,50 , 10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNeuralNetwork(input, output, hiddenLayerSize, batchSize, learningRate,numberOfIterations):\n",
    "    \"\"\" Train neural network\"\"\"\n",
    "    inputSize = input.shape[1]\n",
    "    outputSize = output.shape[1]\n",
    "    errors = np.zeros(numberOfIterations)\n",
    "    \n",
    "    # Initialisation des poids\n",
    "    weigths0 = np.random.rand(inputSize, hiddenLayerSize)/batchSize\n",
    "    weigths1 = np.random.rand(hiddenLayerSize, outputSize)/batchSize\n",
    "    \n",
    "    # Algorithme\n",
    "    for i in range(numberOfIterations):\n",
    "        n = np.random.randint(0,input.shape[0] - batchSize +1)\n",
    "        # Calcul de la sortie de chaque couche \n",
    "        # Y l = f(qN i=1 WilXil)\n",
    "        layer0 = input[n:n + batchSize, :]\n",
    "        layer1 = sigmoid(layer0.dot(weigths0))\n",
    "        layer2 = sigmoid(layer1.dot(weigths1))\n",
    "        \n",
    "        \n",
    "        # Calcul de l’erreur d’apprentissage El\n",
    "        layer2Error = output[n:n + batchSize, :] - layer2\n",
    "        errors[i] = np.mean(np.sum(abs(layer2Error),1))\n",
    "        \n",
    "        # Rétropropagation de l’erreur par descente de gradient\n",
    "        layer2Delta = layer2Error*sigmoid(layer1.dot(weigths1),True)\n",
    "        layer1Error = layer2Delta.dot(weigths1.transpose())\n",
    "        layer1Delta = layer1Error*sigmoid(layer0.dot(weigths0),True)\n",
    "        \n",
    "        # Mise à jour des poids Wli\n",
    "        weigths0 += learningRate*layer0.transpose().dot(layer1Delta)\n",
    "        weigths1 += learningRate*layer1.transpose().dot(layer2Delta)\n",
    "    \n",
    "    return weigths0,weigths1,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00338406, 0.01745264, 0.01210508, ..., 0.01244565, 0.01839823,\n",
       "         0.00518219],\n",
       "        [0.00891415, 0.0038443 , 0.00054418, ..., 0.013169  , 0.00866591,\n",
       "         0.01283   ],\n",
       "        [0.01266213, 0.01366004, 0.00105164, ..., 0.01135148, 0.00211904,\n",
       "         0.01083788],\n",
       "        ...,\n",
       "        [0.01682238, 0.00129871, 0.0009608 , ..., 0.012013  , 0.01582489,\n",
       "         0.01713862],\n",
       "        [0.00845709, 0.0118861 , 0.01454131, ..., 0.01844406, 0.01962531,\n",
       "         0.0093746 ],\n",
       "        [0.01481772, 0.00588983, 0.00887673, ..., 0.01470143, 0.00804727,\n",
       "         0.00545715]]),\n",
       " array([[-41.53497115, -33.44446543, -47.5712246 , -36.34823277,\n",
       "         -40.78514382, -44.55927826, -34.04218305, -38.3930395 ,\n",
       "         -41.60184188, -45.58233091],\n",
       "        [-41.35715688, -33.39467011, -47.37929442, -36.10426717,\n",
       "         -40.73259579, -44.40138094, -34.05856795, -38.09873018,\n",
       "         -41.38432844, -45.34710703],\n",
       "        [-41.43761364, -33.84389031, -47.6726482 , -36.53504699,\n",
       "         -40.86876067, -44.66313968, -34.15809796, -38.19267177,\n",
       "         -41.71787569, -45.57952935],\n",
       "        [-41.5030739 , -33.64590055, -47.61886114, -36.33833668,\n",
       "         -40.89534647, -44.62729517, -34.19139178, -38.32973952,\n",
       "         -41.6162403 , -45.58453144],\n",
       "        [-40.77705608, -33.22090651, -46.82107891, -35.8116153 ,\n",
       "         -40.17369749, -43.81675235, -33.63339209, -37.64769309,\n",
       "         -40.94397053, -44.84129832],\n",
       "        [-40.93569424, -33.23947429, -46.9593279 , -35.93137784,\n",
       "         -40.24428774, -43.99324335, -33.79352441, -37.63090878,\n",
       "         -41.00632262, -44.91170322],\n",
       "        [-41.58344618, -33.66642894, -47.67900157, -36.52516641,\n",
       "         -40.90269918, -44.69489254, -34.15669984, -38.33828533,\n",
       "         -41.73977437, -45.61908637],\n",
       "        [-41.5195866 , -33.68476874, -47.55918203, -36.42591688,\n",
       "         -40.71801319, -44.59682941, -34.21249114, -38.22276983,\n",
       "         -41.58466103, -45.56613962],\n",
       "        [-41.32095874, -33.43046612, -47.37889584, -36.16909211,\n",
       "         -40.66948563, -44.43962057, -33.95020466, -38.13046944,\n",
       "         -41.44918544, -45.41872089],\n",
       "        [-41.54641102, -33.58959144, -47.63422292, -36.43313796,\n",
       "         -40.82145477, -44.6789539 , -34.089943  , -38.41601425,\n",
       "         -41.56494886, -45.62922427],\n",
       "        [-40.86428007, -33.21909228, -46.84313497, -35.72499261,\n",
       "         -40.11803591, -43.90299199, -33.84411238, -37.55302635,\n",
       "         -41.02044758, -44.86054949],\n",
       "        [-41.33353475, -33.44142032, -47.35157574, -36.07744352,\n",
       "         -40.62892233, -44.34682286, -34.0998154 , -38.0730414 ,\n",
       "         -41.44636839, -45.326596  ],\n",
       "        [-41.52356637, -33.61206676, -47.6149709 , -36.21006391,\n",
       "         -40.89613887, -44.59326781, -34.24276299, -38.38883966,\n",
       "         -41.63039056, -45.60602861],\n",
       "        [-42.02787231, -34.04290628, -48.18280915, -36.8693347 ,\n",
       "         -41.4088618 , -45.22922404, -34.66172952, -38.65700949,\n",
       "         -42.12260936, -46.16326291],\n",
       "        [-41.52928864, -33.7159043 , -47.57914171, -36.34211778,\n",
       "         -40.84975292, -44.6034406 , -34.31636618, -38.27625248,\n",
       "         -41.60615087, -45.58071371],\n",
       "        [-41.64160864, -33.60939426, -47.68446138, -36.37891731,\n",
       "         -41.04013604, -44.6512591 , -34.27051939, -38.40233899,\n",
       "         -41.63399292, -45.68476249],\n",
       "        [-41.96318717, -33.80192261, -48.12266092, -36.744263  ,\n",
       "         -41.24685005, -45.06848422, -34.59620558, -38.67753294,\n",
       "         -42.01192546, -46.06757787],\n",
       "        [-42.0560193 , -34.17463728, -48.27009368, -36.83560142,\n",
       "         -41.34914208, -45.13636488, -34.67661427, -38.83248248,\n",
       "         -42.16597607, -46.25642742],\n",
       "        [-41.15117547, -33.53778677, -47.26108854, -36.10289095,\n",
       "         -40.59601878, -44.3239696 , -33.92426486, -37.95036518,\n",
       "         -41.29744008, -45.21106166],\n",
       "        [-41.19328357, -33.50429516, -47.27933187, -36.15045317,\n",
       "         -40.59988255, -44.26417559, -33.90897497, -38.26655611,\n",
       "         -41.36044621, -45.34836223],\n",
       "        [-41.61555195, -33.48521827, -47.7312792 , -36.38251298,\n",
       "         -40.95657293, -44.67410477, -34.36129912, -38.38182535,\n",
       "         -41.68236174, -45.68517745],\n",
       "        [-41.55779589, -33.60764425, -47.56891537, -36.45144995,\n",
       "         -40.64213507, -44.5759781 , -34.33589871, -38.05224101,\n",
       "         -41.60499361, -45.53350024],\n",
       "        [-41.55336397, -33.78901896, -47.62785766, -36.31598504,\n",
       "         -40.94437164, -44.52100587, -34.1640967 , -38.39400274,\n",
       "         -41.62677238, -45.62770676],\n",
       "        [-41.28016236, -33.63255077, -47.4350204 , -36.20923309,\n",
       "         -40.57146875, -44.37870548, -34.04180031, -38.20147682,\n",
       "         -41.43563804, -45.42250167],\n",
       "        [-41.40006327, -33.38205236, -47.45913825, -36.31134855,\n",
       "         -40.78202934, -44.49832635, -34.21889288, -38.1458961 ,\n",
       "         -41.38593085, -45.41606597],\n",
       "        [-41.57447383, -33.84122679, -47.70221175, -36.49182462,\n",
       "         -40.95919644, -44.69862737, -34.12968292, -38.35157756,\n",
       "         -41.71637652, -45.69409916],\n",
       "        [-41.03414971, -33.26865194, -47.06365567, -36.06467711,\n",
       "         -40.34383203, -43.98987533, -33.81854266, -37.98079142,\n",
       "         -41.13357014, -45.1008818 ],\n",
       "        [-41.58267314, -33.67311781, -47.72631923, -36.57205162,\n",
       "         -40.93058831, -44.70036754, -34.23156015, -38.38626428,\n",
       "         -41.67201573, -45.65701756],\n",
       "        [-41.67900617, -33.82117624, -47.80420302, -36.5014783 ,\n",
       "         -40.98293095, -44.72615403, -34.37833168, -38.37689795,\n",
       "         -41.82219726, -45.7714703 ],\n",
       "        [-42.60767435, -34.45191399, -48.82580719, -37.2331626 ,\n",
       "         -41.91143526, -45.72324599, -34.99026503, -39.34017126,\n",
       "         -42.66763041, -46.71921431]]),\n",
       " array([5.46732801, 4.9999732 , 4.99996023, 4.99978802, 4.99630022]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData, trainLabels, testData, testLabels = readMnist(\"Data\\\\MNIST\")\n",
    "trainNeuralNetwork(trainData, trainLabels, 30 ,50 , 10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enseignement_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
